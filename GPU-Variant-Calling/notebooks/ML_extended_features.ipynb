{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning methods to predict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor, BaggingRegressor, GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#Importing data\n",
    "metadata = pd.read_csv('metadata_labeled.csv')\n",
    "metadata = metadata.drop('Label',axis=1)\n",
    "multiqc = pd.read_csv('multiqc_data.csv')\n",
    "label= pd.read_csv('labels.csv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge metad and execution dataframe on Run\n",
    "df = pd.merge(metadata, multiqc, on='Run', how='inner')\n",
    "#extract for M0 from label dataframe along with Run column\n",
    "label= label.iloc[:,[0,1]]\n",
    "#rename the last column as label\n",
    "label.columns = ['Run','Label']\n",
    "#merge metadata and label dataframe on Run\n",
    "metadata = pd.merge(metadata,label, on='Run', how='inner')\n",
    "#Merge df and label dataframe on Run\n",
    "df = pd.merge(df, label, on='Run', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101, 386)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Merging multiqc and metadata\n",
    "merged = pd.merge(metadata, multiqc,on='Run')\n",
    "#Removing columns with no data\n",
    "merged = merged.dropna(axis=1, how='all')\n",
    "#Removing columns with only one unique value\n",
    "merged = merged.loc[:,merged.apply(pd.Series.nunique) != 1]\n",
    "#Removing columns with more than 40% missing values\n",
    "merged = merged.loc[:, merged.isnull().mean() < .4]\n",
    "#Removing columns with more than 40% zeros\n",
    "merged = merged.loc[:, (merged == 0).mean() < .4]\n",
    "#Remove non-numeric columns\n",
    "merged = merged.select_dtypes(include=[np.number])\n",
    "#put Label column at the end in merged dataframe\n",
    "merged = merged[[c for c in merged if c not in ['Label']] + ['Label']]\n",
    "merged.head()\n",
    "#Replace nan with mean of that column\n",
    "merged = merged.fillna(merged.mean())\n",
    "#Extract features which has 0.5 correlation with Label\n",
    "# correlation = merged.corr()\n",
    "# correlation = correlation['Label'].sort_values(ascending=False)\n",
    "# features = correlation.index[abs(correlation)>0.7]\n",
    "# features = features.drop('Label')\n",
    "# df=merged[features]\n",
    "# #add labels to the dataframe\n",
    "# df['Label'] = merged['Label']\n",
    "df = merged\n",
    "#Remove size_MB from df\n",
    "# df = df.drop('size_MB', axis=1)\n",
    "#Remove size_MB from features\n",
    "# features = features.drop('size_MB')\n",
    "#extract only size_MB and Label in df\n",
    "# df = df[['size_MB','Label']]\n",
    "df.shape\n",
    "# features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Vislualize the data\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# #pearson correlation between the variables of merged_new\n",
    "# # merged_new2=merged_new.drop(columns=['Run'])\n",
    "# correlation = data.corr()\n",
    "# plt.figure(figsize=(50,50))\n",
    "# sns.heatmap(correlation, vmax=1, square=True, annot=True, cmap='cubehelix')\n",
    "# plt.title('Correlation between different features')\n",
    "# plt.show()\n",
    "df.to_csv('Processed_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X= df.drop('Label', axis=1)\n",
    "# y = df['Label']\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# #Feature scaling\n",
    "# sc = StandardScaler()\n",
    "# X_train = sc.fit_transform(X_train)\n",
    "# X_test = sc.transform(X_test)\n",
    "\n",
    "# #Train the model\n",
    "# # Define the models\n",
    "# models = [\n",
    "#     ('Linear Regression', LinearRegression()),\n",
    "#     ('Gradient Boosting', GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=1, random_state=0)),\n",
    "#     ('Lasso Regression', Lasso()),\n",
    "#     ('Ridge Regression', Ridge()),\n",
    "#     ('SVM', SVR()),\n",
    "#     ('Random Forest', RandomForestRegressor()),\n",
    "#     ('Decision Tree', DecisionTreeRegressor()),\n",
    "#     ('Neural Network', MLPRegressor(max_iter=50000)),\n",
    "#     ('XGBoost', XGBRegressor())\n",
    "# ]\n",
    "\n",
    "# # Define a function to perform cross-validation and calculate metrics\n",
    "# # def cross_val_and_evaluate(model, X, y):\n",
    "# #     scores_mae = cross_val_score(model, X, y, scoring='neg_mean_absolute_error', cv=10)\n",
    "# #     scores_mse = cross_val_score(model, X, y, scoring='neg_mean_squared_error', cv=10)\n",
    "# #     scores_r2 = cross_val_score(model, X, y, scoring='r2', cv=10)\n",
    "# #     mean_mae = -1 * scores_mae.mean()\n",
    "# #     mean_mse = -1 * scores_mse.mean()\n",
    "# #     mean_r2 = scores_r2.mean()\n",
    "# #     return mean_mae, mean_mse, mean_r2\n",
    "\n",
    "# def cross_val_and_evaluate(model, X, y):\n",
    "#     scores_mae = cross_val_score(model, X, y, scoring='neg_mean_absolute_error', cv=10, error_score='raise')\n",
    "#     scores_mse = cross_val_score(model, X, y, scoring='neg_mean_squared_error', cv=10, error_score='raise')\n",
    "#     scores_r2 = cross_val_score(model, X, y, scoring='r2', cv=10, error_score='raise')\n",
    "#     mean_mae = -1 * scores_mae.mean()\n",
    "#     mean_mse = -1 * scores_mse.mean()\n",
    "#     mean_r2 = scores_r2.mean()\n",
    "#     return mean_mae, mean_mse, mean_r2\n",
    "\n",
    "# # Define a function to display the metrics in a table\n",
    "# def display_results(results):\n",
    "#     print(f\"{'Model':<20}{'MAE':<10}{'MSE':<10}{'R2':<10}\")\n",
    "#     for name, metrics in results.items():\n",
    "#         mean_mae, mean_mse, mean_r2 = metrics\n",
    "#         print(f\"{name:<20}{mean_mae:<10.2f}{mean_mse:<10.2f}{mean_r2:<10.2f}\")\n",
    "# # Train each model, perform cross-validation, calculate the metrics, and add the results to the table\n",
    "# results = {}\n",
    "# for name, model in models:\n",
    "#     results[name] = cross_val_and_evaluate(model, X, y)\n",
    "\n",
    "# #sort the results based on mean R2 value\n",
    "# results_sorted = dict(sorted(results.items(), key=lambda item: item[1][2], reverse=True))\n",
    "# display_results(results_sorted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model               MAE       MSE       R2        \n",
      "Random Forest       4.57      34.15     0.95      \n",
      "Decision Tree       5.10      39.38     0.95      \n",
      "XGBoost             5.05      43.68     0.94      \n",
      "Lasso Regression    10.69     218.21    0.70      \n",
      "Ridge Regression    14.02     478.80    0.35      \n",
      "SVM                 20.27     574.65    0.21      \n",
      "Neural Network      15.33     638.83    0.13      \n",
      "Linear Regression   17.22     801.31    -0.09     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"['spots', 'bases', 'spots_with_mates', 'avgLength', 'size_MB',\\n       'InsertSize', 'psl_0', 'None_mqc-generalstats-fastqc-total_sequences',\\n       'Unique Reads', 'psgc_41', 'psgc_45', 'psgc_49', 'psgc_42', 'psgc_40',\\n       'psgc_46', 'psgc_44', 'psgc_48', 'psgc_39', 'psgc_53', 'psgc_50',\\n       'psgc_43', 'psgc_38', 'psgc_47', 'psgc_52', 'pss_1', 'psgc_37',\\n       'psgc_51', 'psgc_54', 'psgc_36', 'psgc_35', 'Duplicate Reads',\\n       'psgc_34', 'psgc_55', 'psgc_56', 'psgc_33', 'psgc_57', 'pss_0',\\n       'psgc_32', 'psgc_31', 'psgc_30', 'psgc_58', 'psgc_0', 'psgc_29',\\n       'pss_13']\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Model comparison\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Define the models\n",
    "models = [\n",
    "    ('Linear Regression', LinearRegression()),\n",
    "    ('Lasso Regression', Lasso()),\n",
    "    ('Ridge Regression', Ridge()),\n",
    "    ('SVM', SVR()),\n",
    "    ('Random Forest', RandomForestRegressor()),\n",
    "    ('Decision Tree', DecisionTreeRegressor()),\n",
    "    ('Neural Network', MLPRegressor(max_iter=50000)),\n",
    "    ('XGBoost', XGBRegressor())\n",
    "]\n",
    "\n",
    "X=metadata[['spots', 'bases', 'spots_with_mates', 'avgLength', 'size_MB', 'InsertSize']]\n",
    "#Merge X and df to get the final dataframe\n",
    "X=pd.merge(X, df, left_index=True, right_index=True)\n",
    "#Remove duplicate columns from X and Label column\n",
    "X = X.loc[:,~X.columns.duplicated()]\n",
    "#Encode the Label column\n",
    "labelencoder = LabelEncoder()\n",
    "X['Label'] = labelencoder.fit_transform(X['Label'])\n",
    "y=X['Label']\n",
    "X=X.drop('Label', axis=1)\n",
    "\n",
    "# X= df.drop('Label', axis=1)\n",
    "# y = df['Label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "#Scale the data\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "# Define a function to train a model and calculate metrics\n",
    "def train_and_evaluate(model, X_train, y_train, X_test, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    try:\n",
    "        feature_importance = model.feature_importances_\n",
    "        # print('Feature importance:', feature_importance)\n",
    "        \n",
    "    except AttributeError:\n",
    "        feature_importance = None\n",
    "    return mae, mse, r2, feature_importance\n",
    "\n",
    "# Define a function to display the metrics in a table\n",
    "#dipslay feature importance in decimal by two decimal places\n",
    "def display_results(results):\n",
    "    print(f\"{'Model':<20}{'MAE':<10}{'MSE':<10}{'R2':<10}\")\n",
    "    for name, metrics in results.items():\n",
    "        mae, mse, r2, feature_importance = metrics\n",
    "        if feature_importance is not None:\n",
    "            feature_importance = [f\"{value:.2f}\" for value in feature_importance]\n",
    "            feature_importance = ' '.join(feature_importance)\n",
    "        else:\n",
    "            feature_importance = ''\n",
    "        print(f\"{name:<20}{mae:<10.2f}{mse:<10.2f}{r2:<10.2f}\")\n",
    "\n",
    "# Train each model, calculate the metrics, and add the results to the table\n",
    "results = {}\n",
    "for name, model in models:\n",
    "    results[name] = train_and_evaluate(model, X_train, y_train, X_test, y_test)\n",
    "\n",
    "# Display the table\n",
    "# display_results(results)\n",
    "\n",
    "#sort the results based on R2 value\n",
    "results_sorted = dict(sorted(results.items(), key=lambda item: item[1][2], reverse=True))\n",
    "display_results(results_sorted)\n",
    "\n",
    "'''['spots', 'bases', 'spots_with_mates', 'avgLength', 'size_MB',\n",
    "       'InsertSize', 'psl_0', 'None_mqc-generalstats-fastqc-total_sequences',\n",
    "       'Unique Reads', 'psgc_41', 'psgc_45', 'psgc_49', 'psgc_42', 'psgc_40',\n",
    "       'psgc_46', 'psgc_44', 'psgc_48', 'psgc_39', 'psgc_53', 'psgc_50',\n",
    "       'psgc_43', 'psgc_38', 'psgc_47', 'psgc_52', 'pss_1', 'psgc_37',\n",
    "       'psgc_51', 'psgc_54', 'psgc_36', 'psgc_35', 'Duplicate Reads',\n",
    "       'psgc_34', 'psgc_55', 'psgc_56', 'psgc_33', 'psgc_57', 'pss_0',\n",
    "       'psgc_32', 'psgc_31', 'psgc_30', 'psgc_58', 'psgc_0', 'psgc_29',\n",
    "       'pss_13']'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'size_MB_y': 0.706482802096668,\n",
       " 'psgcp_6': 0.08051976936244708,\n",
       " 'psgcp_64': 0.07678833612043111,\n",
       " 'psgcp_74': 0.059680048898120695,\n",
       " 'bases_y': 0.01442709178638129,\n",
       " 'psgc_92': 0.013738603204160189,\n",
       " 'psgc_53': 0.011326673033439176,\n",
       " 'pbsq_16': 0.006251641710128301,\n",
       " 'pss_17': 0.0051474393667803806,\n",
       " 'pbsq_44': 0.003749839512502308}"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extract top 10 importance features for best model above\n",
    "model = DecisionTreeRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "feature_importance = model.feature_importances_\n",
    "features = X.columns\n",
    "features_importance = dict(zip(features, feature_importance))\n",
    "features_importance = dict(sorted(features_importance.items(), key=lambda item: item[1], reverse=True))\n",
    "features_importance = dict(list(features_importance.items())[:10])\n",
    "features_importance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the top 10 features to train the model and calculate the metrics\n",
    "top_features = X[['size_MB_y', 'psgcp_6', 'psgcp_64', 'psgcp_74', 'bases_y', 'psgc_92', 'psgc_53', 'pbsq_16', 'pss_17', 'pbsq_44']]\n",
    "#extract only top_features and Label \n",
    "top_features['Label'] = y\n",
    "#predict using top_features\n",
    "X=top_features.drop('Label', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save results_sorted to a csv file and keep the rows name \n",
    "results_sorted_df = pd.DataFrame(results_sorted)\n",
    "#rename rows as Models, MAE, MSE, R2, Feature Importance\n",
    "results_sorted_df = results_sorted_df.rename(index={0: 'MAE', 1: 'MSE', 2: 'R2', 3: 'Feature Importance'})\n",
    "results_sorted_df.to_csv('Model_comparison.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75.38333333333334\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
